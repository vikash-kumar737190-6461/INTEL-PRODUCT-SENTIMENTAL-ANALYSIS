


import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import os

urls = [
    'https://www.amazon.in/Intel-Generation-Desktop-Processor-Warranty/dp/B09MDFH5HY/ref=sr_1_1?dib=eyJ2IjoiMSJ9.aG27SdaOhKl1C6IQGZWQVJB1NN1gT6Hj4DLkrO9cl3r_7COiobbVC2hz5c3Zyir1waCgOlpau_oGvCv8vhKq7vZFd9EWEBylEqeGYAGl9Xcwlct-AhscbQ_AGC5q_mq7rrztO6YhuhidIl9jv0GywH8L1VaBcKQUbRvrNhT9sFHeDqlu53Omw27dYOP0QhL8R_KI-QCJhoGXB8NVGNqU2-ejHm5I3URPLkIe23Jy9-w.ro4GwKXwROuzejcOgbHqRgSmdP7sk8f9woc3yOKuk9M&dib_tag=se&keywords=intel+13th+gen+processor&qid=1719047314&sr=8-1',  # 13th Gen Intel® Core desktop processors
    'https://www.amazon.in/i7-14700K-Desktop-Processor-Integrated-Graphics/dp/B0CGJ41C9W/ref=sr_1_2?crid=EFVNECX8WUIV&dib=eyJ2IjoiMSJ9.OQfGGdOmWi7_ZwojYpOcOIkblkhi8BAXnOwaKVOhqiGybM9YlNlr_hw4jxAquueKzziF2A9QpO0yPgzRFZQ5fCwkqmhy6I4zHHFxIceyAtEpWZ4cHp1Tls2GKw9hJMkS359Ug_eu3PbTsFP-CvpPlQueivxUqWT7VKU-_C1CkHtjx70_WjLOrDAtdO-nUjdCWUHDfIkpTnYbDZe6nAMy1zrcC0-bV0u-SLZvH4KA5lM.RlGeQMcvfRvYL4uz17u5aKzxDnapqfqLj693Lfw_Z8A&dib_tag=se&keywords=Intel%C2%AE+Core+desktop+processors+%2814th+gen%29&qid=1719047434&sprefix=intel+core+desktop+processors+14th+gen+%2Caps%2C224&sr=8-2',  # 14th Gen Intel® Core desktop processors
    'https://www.amazon.in/Intel-Generation-Desktop-Processor-Warranty/dp/B09MDGKQLY/ref=sr_1_1?crid=2NCOX2O7VS6N9&dib=eyJ2IjoiMSJ9.HhlMGoRTC6Ufyu1C-zRS-u8u2xtnQEXSNE7HiloC4RoFPe5tbU3z_i5V3aE6Jtlu1P8SIJg8drQZyn5dizUEMA5wbvx_mArm_FFjGw4Cj3mEPr_WHACUtOuOeAmR0H5bpjSW-v9zKKsVEzObYIQ5WyyEVODFKjtIoc3nPsrcU-94PK_qMkLMe-OIlymai6O-hZD6eYlJt7BeXblbvCzuuORt0UV5-VFgz6sUmTixPGA._9BsXXaENinzjo9gbPaY7aQsdoXmTfiE1ax3LWNl8Oc&dib_tag=se&keywords=12th+Gen+Intel%C2%AE+Core+desktop+processors&qid=1719047555&sprefix=12th+gen+intel+core+desktop+processors%2Caps%2C216&sr=8-1',  # 12th Gen Intel® Core desktop processors
    'https://www.amazon.in/i7-2670QM-SR02N-PGA988B-Mobile-Processor/dp/B008RET4D0/ref=sr_1_1?crid=2ZZAX73P6PUQ7&dib=eyJ2IjoiMSJ9.Wmbh3zHgpcXRytKgw8qJXkYR4KwM_5FIAPbIKJacrm4AcDZZrfn4AAZ7Okk7xRxGMqWPdrQsklODL1ci5aOdhQcfOtzTb8yoyPRk0UfigapO_SpVKfCUF9xoTCOUw_HkNKh55JWCUkn3vVebTS-pA1EqgY6nDVtMROhl4NOwA8r8u2f1YkTiiqjqu9oECx0p-pDfXlMjcbtwb4agDQgK6xq6dws3sESdYBEDmbtFfCI.jHyjMX8Kx9KeeqbUv6Lw4zPfhl-qdL5mzJk9v2ILRfU&dib_tag=se&keywords=intel+mobile+processor&qid=1719047597&sprefix=intel+mobile+process%2Caps%2C216&sr=8-1',  # 12th Gen Intel® Core mobile processors
    'https://www.amazon.in/Intel%C2%AE-CoreTM-i5-13400-Processor-Cache/dp/B0BN68JXR2/ref=sr_1_16?crid=2MD5O16N41NAB&dib=eyJ2IjoiMSJ9.Wmbh3zHgpcXRytKgw8qJXkYR4KwM_5FIAPbIKJacrm4AcDZZrfn4AAZ7Okk7xRxGMqWPdrQsklODL1ci5aOdhQcfOtzTb8yoyPRk0UfigapO_SpVKfCUF9xoTCOUw_HkNKh55JWCUkn3vVebTS-pA1EqgY6nDVtMROhl4NOwA8r8u2f1YkTiiqjqu9oECx0p-pDfXlMjcbtwb4agDQgK6xq6dws3sESdYBEDmbtFfCI.jHyjMX8Kx9KeeqbUv6Lw4zPfhl-qdL5mzJk9v2ILRfU&dib_tag=se&keywords=intel+mobile+processor&qid=1719047667&sprefix=intel+mobile+processor%2Caps%2C202&sr=8-16'  # 13th Gen Intel® Core mobile processors
]

# Set up headers with a user-agent to mimic a browser
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'en-US, en;q=0.5'
}

# Function to get the HTML content of a page
def get_page_content(url, headers, retries=3):
    for i in range(retries):
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            return response.content
        except requests.exceptions.HTTPError as e:
            print(f"HTTPError: {e}")
            if i < retries - 1:
                print("Retrying...")
                time.sleep(random.randint(1, 3))
            else:
                raise
        except requests.exceptions.RequestException as e:
            print(f"RequestException: {e}")
            raise

# Function to parse the reviews from the page content
def parse_reviews(page_content):
    soup = BeautifulSoup(page_content, 'html.parser')
    reviews = soup.find_all('div', {'data-hook': 'review'})
    
    review_data = []
    for review in reviews:
        try:
            title = review.find('a', {'data-hook': 'review-title'}).get_text(strip=True)
        except AttributeError:
            title = None
        try:
            rating = review.find('i', {'data-hook': 'review-star-rating'}).get_text(strip=True)
        except AttributeError:
            rating = None
        try:
            date = review.find('span', {'data-hook': 'review-date'}).get_text(strip=True)
        except AttributeError:
            date = None
        try:
            content = review.find('span', {'data-hook': 'review-body'}).get_text(strip=True)
        except AttributeError:
            content = None
        
        review_data.append({
            'title': title,
            'rating': rating,
            'date': date,
            'content': content
        })
    
    return review_data

# Function to get reviews from multiple pages
def get_reviews(url, headers, min_reviews=200):
    reviews = []
    page = 1
    while len(reviews) < min_reviews:
        print(f"Fetching page {page} of reviews for URL: {url}")
        page_url = f"{url}&pageNumber={page}"
        page_content = get_page_content(page_url, headers)
        new_reviews = parse_reviews(page_content)
        if not new_reviews:
            break
        reviews.extend(new_reviews)
        page += 1
        time.sleep(random.randint(1, 3))  # Respectful scraping delay
    return reviews

all_reviews = []
for url in urls:
    product_reviews = get_reviews(url, headers, min_reviews=200)
    all_reviews.extend(product_reviews)

# Create DataFrame from collected reviews
reviews_df = pd.DataFrame(all_reviews)

# Save to CSV
output_file = 'intel_amazon_reviews.csv'
if os.path.exists(output_file):
    os.remove(output_file)
reviews_df.to_csv(output_file, index=False)





import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import shap
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

# Load and inspect the dataset
df = pd.read_csv('intel_amazon_reviews.csv')
# Extract numerical ratings
def extract_rating(rating_str):
    try:
        return float(rating_str.split()[0])
    except:
        return None

df['rating'] = df['rating'].apply(extract_rating)

pattern = r'^\d+(\.\d+)? out of 5 stars'
df['title'] = df['title'].str.replace(pattern, '', regex=True).str.strip()

def extract_date(date):
    try:
        # Split the date string and extract the date part
        date_parts = date.split(" on ")
        if len(date_parts) > 1:
            return date_parts[1]
        else:
            return None
    except Exception as e:
        print(f"Error processing date: {date} - {e}")
        return None


# Apply the function to the 'date' column
df['date'] = df['date'].apply(extract_date)

# Convert the extracted date strings to datetime objects
df['date'] = pd.to_datetime(df['date'], format='%d %B %Y')

# Clean text
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'\W', ' ', text)  # Remove non-word characters
    text is text.lower()  # Convert to lowercase
    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords
    return text

df['cleaned_content'] = df['content'].apply(clean_text)
df.drop("content", axis =1, inplace=True)
df.head()





# Sentiment analysis
analyzer = SentimentIntensityAnalyzer()
df['sentiment_score'] = df['cleaned_content'].apply(lambda x: analyzer.polarity_scores(x)['compound'])
df['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))
print(df[['cleaned_content', 'sentiment_score', 'sentiment']].head())

# Prepare data for machine learning
df = df[df['sentiment'] != 'neutral']
df['label'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)

vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['cleaned_content']).toarray()
y = df['label']





X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"X_train shape: {X_train.shape}, X_test shape : {X_test.shape}")





# Logistic Regression model
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)
y_pred = logistic_model.predict(X_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Feature importance for logistic regression
feature_importance = np.abs(logistic_model.coef_[0])
sorted_indices = np.argsort(feature_importance)[::-1]
top_features = [vectorizer.get_feature_names_out()[i] for i in sorted_indices[:10]]
print("Top Features:", top_features)





# Deep Learning model
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df['cleaned_content'])
sequences = tokenizer.texts_to_sequences(df['cleaned_content'])
X = pad_sequences(sequences, maxlen=100)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))
model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))

loss, accuracy = model.evaluate(X_test, y_test)
print("LSTM Model Accuracy:", accuracy)





# SHAP interpretation for LSTM model
explainer = shap.KernelExplainer(model.predict, X_train[:100])
shap_values = explainer.shap_values(X_test[:10], nsamples=100)

shap.initjs()

# Flatten shap_values to match X_test
shap_values = np.array(shap_values).squeeze()  

# Visualize SHAP values for each sample
for i in range(len(shap_values)):
    shap.force_plot(explainer.expected_value, shap_values[i], X_test[i], feature_names=[str(j) for j in range(100)])





import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])
disp.plot()
plt.title("Confusion Matrix for Logistic Regression")
plt.show()





import pickle as pkl
import os
## LSTM model
model.save("lstm.keras")

## Logistic Rgression model
file = open("logistic.pkl", 'wb')
pkl.dump(logistic_model, file)


# to save cleaned data
df.to_csv("cleaned_data.csv")


df.head()


from tensorflow.keras.models import load_model
model = load_model("lstm.keras")






